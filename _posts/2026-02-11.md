좋은 주제다.

- *“LLM을 Playwright로 평가한다”**는 건 결국 브라우저 기반으로 LLM을 호출하고, 그 결과를 자동으로 수집·채점하는 구조를 만드는 거다.

SQA 관점에서 보면 이건 **UI 테스트 + 모델 품질 검증 자동화**를 결합하는 작업이다.

---

# 1️⃣ 먼저 구조부터 정리하자

LLM 평가 자동화 구조는 보통 이렇게 간다:

```
Test Case (프롬프트)
        ↓
Playwright
        ↓
LLM UI or API 호출
        ↓
응답 수집
        ↓
검증 로직 (정답 비교 or 평가 모델)
        ↓
리포트 생성
```

---

# 2️⃣ 평가 방식부터 결정해야 한다

Playwright는 단순 자동화 도구다.

핵심은 **“무엇을 기준으로 평가할 것인가”**다.

LLM 평가는 보통 4가지로 나뉜다.

### ① Exact Match (정답 비교)

정해진 정답과 동일한지 확인

→ FAQ 챗봇, 정책 응답 검증에 적합

---

### ② 키워드 기반 평가

특정 키워드가 포함되어 있는지 확인

→ 예:

- “환불” 질문
- 응답에 “7일 이내”, “영수증 필요” 포함 여부 체크

---

### ③ 규칙 기반 평가

- 금지어 포함 여부
- 개인정보 노출 여부
- 문장 길이 제한
- JSON 포맷 유효성

---

### ④ LLM-as-a-Judge (모델이 모델을 평가)

응답을 또 다른 LLM에게 보내서 점수화

```
"다음 답변이 질문에 적절한지 1~5점으로 평가해라"
```

이게 요즘 가장 많이 쓰는 방식.

---

# 3️⃣ Playwright로 구현하는 방법

## ✅ 방법 1: 웹 챗봇 UI를 직접 테스트

예: 사내 챗봇 콘솔

```tsx
import { test, expect } from '@playwright/test';

test('LLM 응답 평가 테스트', async ({ page }) => {

  await page.goto('https://your-chatbot-url.com');

  // 프롬프트 입력
  await page.fill('#chat-input', '환불 규정 알려줘');
  await page.click('#send-button');

  // 응답 기다리기
  const responseLocator = page.locator('.chat-response').last();
  await responseLocator.waitFor();

  const responseText = await responseLocator.innerText();

  // 검증 로직
  expect(responseText).toContain('7일');
  expect(responseText).not.toContain('주민등록번호');
});
```

이건 UI 레벨 검증.

---

## ✅ 방법 2: Playwright에서 API 직접 호출 (추천)

UI 거치지 말고 API 직접 때리는 게 훨씬 안정적이다.

```tsx
import { test, expect } from '@playwright/test';

test('LLM API 평가', async ({ request }) => {

  const response = await request.post('https://api.your-llm.com/chat', {
    data: {
      prompt: "환불 규정 알려줘"
    }
  });

  const body = await response.json();
  const answer = body.result;

  expect(answer).toContain('7일');
});
```

이 방식이:

- 속도 빠름
- UI flaky 없음
- CI/CD에 적합

---

# 4️⃣ 테스트 데이터 구조화

LLM 평가는 **데이터셋 기반 자동 반복**이 핵심이다.

예:

```json
[
  {
    "prompt": "환불 규정 알려줘",
    "expected_keywords": ["7일", "영수증"]
  },
  {
    "prompt": "비밀번호 변경 방법",
    "expected_keywords": ["설정", "보안"]
  }
]
```

테스트 코드에서 loop 돌린다.

---

# 5️⃣ LLM-as-a-Judge 구조 (고급)

```tsx
const judgePrompt = `
질문: ${prompt}
답변: ${answer}

위 답변이 적절한지 1~5점으로 평가하고 이유를 말해라.
`;

const judgeResponse = await request.post('judge-llm-api', { data: { prompt: judgePrompt }});
```

→ 점수 4점 이상이면 통과

이 구조는

- 의미 기반 평가 가능
- Exact match보다 훨씬 현실적

---

# 6️⃣ CI/CD 연동 구조

DevOps 파이프라인에서:

```
PR 생성
  ↓
Playwright LLM 평가 실행
  ↓
품질 기준 미달 → 빌드 실패
```

예:

- 평균 점수 4.0 미만 → fail
- 금지어 포함 → fail
- JSON schema mismatch → fail

---

# 7️⃣ SQA 고도화 관점에서 보면

너가 CSP 콘솔 UI 테스트 하고 있다면

LLM 검증은 이런 방향이 좋다:

### ✔ 단순 UI 테스트 수준에서 벗어나려면

- Prompt Regression Test
- Temperature 변경 시 응답 안정성 측정
- Hallucination 탐지
- 응답 지연 시간 측정
- 토큰 사용량 모니터링

---

# 8️⃣ 현실적으로 가장 추천 구조

1. Playwright + API 호출
2. JSON 데이터셋 기반 반복
3. 키워드 + 금지어 + LLM Judge 혼합 평가
4. 리포트는 HTML 리포트 자동 생성

---

# 9️⃣ 한 단계 더 가면

Playwright만으로는 한계 있다.

LLM 평가 전용 툴:

- LangSmith
- DeepEval
- RAGAS
- Promptfoo

Playwright는 **실행기**

평가 로직은 별도 모듈로 분리하는 게 맞다.

---

# 결론

Playwright는 평가 “플랫폼”이지

평가 “기준”이 아니다.

LLM 평가 자동화 핵심은:

- 무엇을 품질로 정의할 것인가
- 정답이 없는 문제를 어떻게 수치화할 것인가
- 회귀 테스트를 어떻게 만들 것인가

---

원하면 내가

- 실무형 LLM 평가 아키텍처 설계
- SQA팀 도입 전략
- 6개월 고도화 로드맵

이 중 하나를 구체적으로 정리해줄까?
