# 데이터 분석 심화 2 - CNN(Convolutional NN)

## A. Classification for MNIST

### 1. MNIST Dataset 98%이상 달성하기

- 레이어 구성 : 4 layer\
  L1(784, 1024) + Dropout 0.3\
  L2(512, 1024) + Dropout 0.3\
  L3(512, 1024) + Dropout 0.3\
  L4(512, 10)
- Activation Function : ReLU
- Optimizer : Adam
- Batch_size : 100, Epochs : 15


```python
import tensorflow as tf
import matplotlib.pyplot as plt
```


```python
# 데이터 정의
mnist = tf.keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0
```


```python
# 모델 구성
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```


```python
# 모델 컴파일
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=["accuracy"])
model.summary()

# 파라미터 개수 = 필터*입력채널*출력채널+출력크기

# 모델 fit
hist = model.fit(X_train, Y_train,
                 validation_data=(X_test, Y_test),
                 verbose=2, batch_size=100, epochs=15,
                 use_multiprocessing=True)

# 모델 평가
model.evaluate(X_test, Y_test,verbose=2, batch_size=100, use_multiprocessing=True)
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten (Flatten)            (None, 784)               0         
    _________________________________________________________________
    dense (Dense)                (None, 1024)              803840    
    _________________________________________________________________
    dropout (Dropout)            (None, 1024)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 1024)              1049600   
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 1024)              0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 1024)              1049600   
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 1024)              0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 512)               524800    
    _________________________________________________________________
    dense_4 (Dense)              (None, 10)                5130      
    =================================================================
    Total params: 3,432,970
    Trainable params: 3,432,970
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/15
    600/600 - 148s - loss: 0.2593 - accuracy: 0.9204 - val_loss: 0.1336 - val_accuracy: 0.9616
    Epoch 2/15
    600/600 - 142s - loss: 0.1269 - accuracy: 0.9631 - val_loss: 0.1055 - val_accuracy: 0.9675
    Epoch 3/15
    600/600 - 101s - loss: 0.1007 - accuracy: 0.9711 - val_loss: 0.0831 - val_accuracy: 0.9754
    Epoch 4/15
    600/600 - 111s - loss: 0.0814 - accuracy: 0.9762 - val_loss: 0.0789 - val_accuracy: 0.9768
    Epoch 5/15
    600/600 - 100s - loss: 0.0714 - accuracy: 0.9791 - val_loss: 0.0978 - val_accuracy: 0.9730
    Epoch 6/15
    600/600 - 109s - loss: 0.0645 - accuracy: 0.9803 - val_loss: 0.0625 - val_accuracy: 0.9817
    Epoch 7/15
    600/600 - 100s - loss: 0.0587 - accuracy: 0.9826 - val_loss: 0.0706 - val_accuracy: 0.9800
    Epoch 8/15
    600/600 - 95s - loss: 0.0538 - accuracy: 0.9843 - val_loss: 0.0736 - val_accuracy: 0.9795
    Epoch 9/15
    600/600 - 112s - loss: 0.0522 - accuracy: 0.9856 - val_loss: 0.0653 - val_accuracy: 0.9815
    Epoch 10/15
    600/600 - 109s - loss: 0.0502 - accuracy: 0.9854 - val_loss: 0.0820 - val_accuracy: 0.9778
    Epoch 11/15
    600/600 - 105s - loss: 0.0453 - accuracy: 0.9868 - val_loss: 0.0691 - val_accuracy: 0.9821
    Epoch 12/15
    600/600 - 112s - loss: 0.0421 - accuracy: 0.9877 - val_loss: 0.0810 - val_accuracy: 0.9836
    Epoch 13/15
    600/600 - 95s - loss: 0.0406 - accuracy: 0.9882 - val_loss: 0.0740 - val_accuracy: 0.9821
    Epoch 14/15
    600/600 - 93s - loss: 0.0349 - accuracy: 0.9903 - val_loss: 0.0768 - val_accuracy: 0.9818
    Epoch 15/15
    600/600 - 110s - loss: 0.0399 - accuracy: 0.9882 - val_loss: 0.0792 - val_accuracy: 0.9839
    100/100 - 5s - loss: 0.0792 - accuracy: 0.9839
    




    [0.07924164831638336, 0.9839000105857849]




```python
# Reporting
# Cost / Accuracy
plt.figure(figsize=(8, 4)) # 8 x 4 inchs
plt.subplot(1, 2, 1)
plt.plot(hist.history['loss'])
plt.title("Cost Graph")
plt.ylabel("cost")
plt.subplot(1, 2, 2)
plt.title("Accuracy Graph")
plt.ylabel("accuracy")
plt.plot(hist.history['accuracy'], 'b-', label="training accuracy")
plt.plot(hist.history['val_accuracy'], 'r:', label="validation accuracy")
plt.legend()
plt.tight_layout()
plt.show()
```


    
![png](output_6_0.png)
    


## B. CNN(합성곱신경망)

### 1. CNN(Convolution Neural Network)
- 입력데이터 : 이미지
- 특징 추출과 분류를 동시에 학습하는 NN
- CNN은 일반적으로 3종류의 Layer로 구성\
  **Convolution Layer**: 의미있는 feature를 추출하는 layer\
  Pooling Layer: feature를 줄이기 위해 subsampling을 수행\
  Feedforward(Dense) Layer: 분류를 위한 classification layer(Neural Network)\

**▶ Convolution Layer**
- 입력된 이미지의 일부분과 필터 사이의 계산이 convolution되면서 수행되는데, 그 결과를 입력으로 받는 Layer
- 대부분의 컬러이미지는 width*height*channel로 구성
- 대부분의 필터 또한 width*height*channel로 구성
- Layer구성: filter를 적용하여 매 번 one number를 얻어내며, 계산결과에 의해 1개의 activation map이 생성
- 적용함수: f x = Wx + b 또는 f x = ReLU(Wx + b)

**▶ Stride**
- Convolutional Layer의 크기는?
- output size = (N-F)/stride + 1
- image size : N x N
- filter size : F x F
- stride : filter를 움직이는 간격
![%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202021-07-16%20101032-2.png](attachment:%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202021-07-16%20101032-2.png)

**▶ Padding**
- output size를 크게 할 수 없을까?
- padding : 이미지의 바깥경계에 0을 추가\
  (예) padding = 1인 경우,\
![%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202021-07-16%20102528-2.png](attachment:%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202021-07-16%20102528-2.png)\
  N=7, F=3, stride=1, padding=1\
  output size = (9-3)/1+1 = 7

**▶ Pooling Layer**
- sampling된 layer
 - Convolutional Layer의 크기를 resize
 - 주어진 layer를 적절하게 subsampling하여 크기를 줄여 줌(주로 max pooling을 사용)
- max pooling: filter안의 값들 중 최대값을 고르는 방법

## C. Stride, Padding, Pooling

### 1. Tensorflow로 간단한 CONV Layer만들기


```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import random
```


```python
image = np.array([[[[1],[2],[3]],
                   [[4],[5],[6]],
                   [[7],[8],[9]]]], dtype=np.float32)

print('image.shape = ', image.shape)  #(1,3,3,1) : (number, row, col, channel)
print('image.reshaped.shape = ', image.reshape(3,3).shape)
print('image.reshaped:\n', image.reshape(3,3), sep='')
plt.imshow(image.reshape(3,3), cmap='gray')
```

    image.shape =  (1, 3, 3, 1)
    image.reshaped.shape =  (3, 3)
    image.reshaped:
    [[1. 2. 3.]
     [4. 5. 6.]
     [7. 8. 9.]]
    




    <matplotlib.image.AxesImage at 0x23f9142a160>




    
![png](output_13_2.png)
    



```python
# Filter 정의(1개)
weight = tf.constant([[[[1.0]],[[1.0]]],[[[1.0]],[[1.0]]]])

print('weight.shape = ', weight.shape)  #(2,2,1,1):(row,col,channel,number)
weight_init = tf.constant_initializer(weight.numpy())
weight_img = tf.reshape(weight, (2,2))
print('weight.reshaped.shape = ', weight_img.shape)
print('weight.reshaped:\n', weight_img.numpy(), sep='')
```

    weight.shape =  (2, 2, 1, 1)
    weight.reshaped.shape =  (2, 2)
    weight.reshaped:
    [[1. 1.]
     [1. 1.]]
    


```python
# Filter 적용하기(padding=‘VALID’)
print("padding='VALID'") # 패딩(바깥여백)을 적용안함
conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=2, padding='VALID',kernel_initializer=weight_init)(image)

conv2d_img = conv2d.numpy()
print('conv2d_img.shape = ', conv2d_img.shape)
print('conv2d_img:\n', conv2d_img, sep='')
print('conv2d_img.reshaped.shape = ', conv2d_img.reshape(2,2).shape)
print('conv2d_img.reshaped:\n', conv2d_img.reshape(2,2), sep='')
```

    padding='VALID'
    conv2d_img.shape =  (1, 2, 2, 1)
    conv2d_img:
    [[[[12.]
       [16.]]
    
      [[24.]
       [28.]]]]
    conv2d_img.reshaped.shape =  (2, 2)
    conv2d_img.reshaped:
    [[12. 16.]
     [24. 28.]]
    


```python
# Filter 적용하기 (padding=‘'SAME')
print("padding='SAME'") # 출력이 입력과 같은 크기가 되도록 패딩을 적용함
conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=2, padding='SAME',kernel_initializer=weight_init)(image)

conv2d_img = conv2d.numpy()
print('conv2d_img.shape = ', conv2d_img.shape)
print('conv2d_img:\n', conv2d_img, sep='')
print('conv2d_img.reshaped.shape = ', conv2d_img.reshape(3,3).shape)
print('conv2d_img.reshaped:\n', conv2d_img.reshape(3,3), sep='')
```

    padding='SAME'
    conv2d_img.shape =  (1, 3, 3, 1)
    conv2d_img:
    [[[[12.]
       [16.]
       [ 9.]]
    
      [[24.]
       [28.]
       [15.]]
    
      [[15.]
       [17.]
       [ 9.]]]]
    conv2d_img.reshaped.shape =  (3, 3)
    conv2d_img.reshaped:
    [[12. 16.  9.]
     [24. 28. 15.]
     [15. 17.  9.]]
    


```python
# 여러 개의 Filter 적용하기
# Filter 정의 (3개)
weight = tf.constant([[[[1.0, 10.0, -1.0]],[[1.0, 10.0, -1.0]]],
                      [[[1.0, 10.0, -1.0]],[[1.0, 10.0, -1.0]]]])

weight_img = weight.numpy()
print('weight.shape = ', weight.shape) # (2,2,1,3)=>(row,col,channel,number)
weight_init = tf.constant_initializer(weight.numpy())
weight_img = np.swapaxes(weight_img, 0, 3)
for i, one_img in enumerate(weight_img):
    print(one_img.reshape(2,2))
```

    weight.shape =  (2, 2, 1, 3)
    [[1. 1.]
     [1. 1.]]
    [[10. 10.]
     [10. 10.]]
    [[-1. -1.]
     [-1. -1.]]
    


```python
# Filter 3개 적용하기(padding='SAME')
print("padding='SAME'")
conv2d = tf.keras.layers.Conv2D(filters=3, kernel_size=2, padding='SAME', kernel_initializer=weight_init)(image)
conv2d_img = conv2d.numpy()
print('conv2d_img.shape = ', conv2d_img.shape)

conv2d_img = np.swapaxes(conv2d_img, 0, 3)
for i, one_img in enumerate(conv2d_img):
    print(one_img.reshape(3,3))
```

    padding='SAME'
    conv2d_img.shape =  (1, 3, 3, 3)
    [[12. 16.  9.]
     [24. 28. 15.]
     [15. 17.  9.]]
    [[120. 160.  90.]
     [240. 280. 150.]
     [150. 170.  90.]]
    [[-12. -16.  -9.]
     [-24. -28. -15.]
     [-15. -17.  -9.]]
    

### 2. Tensorflow로 Max Pooling 구현


```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import random
```


```python
# 간단한 이미지 만들기
image = np.array([[[[1],[2],[3]],
                   [[4],[5],[6]],
                   [[7],[8],[9]]]], dtype=np.float32)

print('image.shape = ', image.shape)
print('image.reshaped.shape = ', image.reshape(3,3).shape)
print('image.reshaped:\n', image.reshape(3,3), sep='')
```

    image.shape =  (1, 3, 3, 1)
    image.reshaped.shape =  (3, 3)
    image.reshaped:
    [[1. 2. 3.]
     [4. 5. 6.]
     [7. 8. 9.]]
    


```python
# Max Pooling
print("padding='SAME'")
pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1,padding='SAME')(image)

pool_img = pool.numpy()
print('pool_img.shape = ', pool_img.shape)
conv2d_img = np.swapaxes(pool_img, 0, 3)
for i, one_img in enumerate(pool_img):
    print(one_img.reshape(3,3))
```

    padding='SAME'
    pool_img.shape =  (1, 3, 3, 1)
    [[5. 6. 6.]
     [8. 9. 9.]
     [8. 9. 9.]]
    

### 3. MNIST 데이터를 이용한 Conv2D, MaxPool2D


```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
```


```python
# MINIST dataset loading & select an image
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.astype(np.float32) / 255.0
X_test = X_test.astype(np.float32) / 255.0

img = X_train[0]
plt.imshow(img, cmap='gray')
plt.show()
```


    
![png](output_25_0.png)
    



```python
# Convolution Layer
img = img.reshape(-1, 28, 28, 1)
img = tf.convert_to_tensor(img)
weight_init = tf.keras.initializers.RandomNormal(stddev=0.01)
conv2d = tf.keras.layers.Conv2D(filters=5, kernel_size=3, strides=(2,2),
                                padding='SAME',
                                kernel_initializer=weight_init)(img)

print(f"conv2d.shape = {conv2d.shape}")

images = np.swapaxes(conv2d, 0, 3)
for i, image in enumerate(images):
    plt.subplot(1, 5, i+1)
    plt.imshow(image.reshape(14,14), cmap='gray')
plt.show()
```

    conv2d.shape = (1, 14, 14, 5)
    


    
![png](output_26_1.png)
    



```python
# Pooling Layer
pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),
                                 padding='SAME')(conv2d)

print(f"pool.shape = {pool.shape}")
images = np.swapaxes(pool, 0, 3)
for i, image in enumerate(images):
    plt.subplot(1, 5, i+1)
    plt.imshow(image.reshape(7,7), cmap='gray')
plt.show()
```

    pool.shape = (1, 7, 7, 5)
    


    
![png](output_27_1.png)
    


## D. CNN for MNIST
CNN의 구성
- Input Layer : 1개
- Convolutional layer : 3개\
  각 layer마다 Pooling layer추가\
  Dropout : 0.3
- Fully-connected Layer : 2개
- Output Layer : 1개


```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import matplotlib.pyplot as plt
```


```python
# 데이터 로딩 / 전처리
mnist = tf.keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)
X_train, X_test = X_train / 255.0, X_test / 255.0

# 모델 정의
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), padding='same',activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1),padding='same', activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Conv2D(128, (3, 3), padding='same',activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),
    
    tf.keras.layers.Flatten(),tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```


```python
# 모델 컴파일
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=["accuracy"])
model.summary()

# 1번째 Conv2D 층
# 3 * 3 (필터크기) * 1 (입력채널) * 32 (출력채널) + 32 = 320

# 2번째 Conv2D 층
# 3 * 3 (필터크기) * 32 (입력채널) * 64 (출력채널) + 64 = 320

# 모델 적합
hist = model.fit(X_train, Y_train,
                 validation_data=(X_test, Y_test),
                 verbose=2, batch_size=100, epochs=5, use_multiprocessing=True)

# 모델 평가
model.evaluate(X_test, Y_test,verbose=2, batch_size=100, use_multiprocessing=True)
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       
    _________________________________________________________________
    max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 14, 14, 32)        0         
    _________________________________________________________________
    conv2d_5 (Conv2D)            (None, 14, 14, 64)        18496     
    _________________________________________________________________
    max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         
    _________________________________________________________________
    dropout_4 (Dropout)          (None, 7, 7, 64)          0         
    _________________________________________________________________
    conv2d_6 (Conv2D)            (None, 7, 7, 128)         73856     
    _________________________________________________________________
    max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 2048)              0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               262272    
    _________________________________________________________________
    dense_6 (Dense)              (None, 10)                1290      
    =================================================================
    Total params: 356,234
    Trainable params: 356,234
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/5
    600/600 - 263s - loss: 0.2451 - accuracy: 0.9216 - val_loss: 0.0516 - val_accuracy: 0.9835
    Epoch 2/5
    600/600 - 263s - loss: 0.0681 - accuracy: 0.9785 - val_loss: 0.0375 - val_accuracy: 0.9874
    Epoch 3/5
    600/600 - 280s - loss: 0.0512 - accuracy: 0.9842 - val_loss: 0.0315 - val_accuracy: 0.9896
    Epoch 4/5
    600/600 - 255s - loss: 0.0396 - accuracy: 0.9874 - val_loss: 0.0267 - val_accuracy: 0.9915
    Epoch 5/5
    600/600 - 229s - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.0292 - val_accuracy: 0.9910
    100/100 - 8s - loss: 0.0292 - accuracy: 0.9910
    




    [0.02922763302922249, 0.9909999966621399]




```python
# Reporting
# Cost / Accuracy
plt.figure(figsize=(8, 4)) # 8 x 4 inchs
plt.subplot(1, 2, 1)
plt.plot(hist.history['loss'])
plt.title("Cost Graph")
plt.ylabel("cost")
plt.subplot(1, 2, 2)
plt.title("Accuracy Graph")
plt.ylabel("accuracy")
plt.plot(hist.history['accuracy'], 'b-', label="training accuracy")
plt.plot(hist.history['val_accuracy'], 'r:', label="validation accuracy")
plt.legend()
plt.tight_layout()
plt.show()
```


    
![png](output_32_0.png)
    


>**MNIST Dataset Classification: 각 메소드 결과**
>- Softmax Classification: 약 92%
>- Neural Network(NN) Classification: 약 98%
>- Convolutional-NN Classification: 약 99%


```python

```
